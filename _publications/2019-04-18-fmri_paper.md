---
title: "Representation of Locomotive Affordances in Brains and Models"
date: 2024-07-14
layout: single
categories:
  - research
tags:
  - publication
  - neuroscience
  - fMRI
  - affordances
  - scene perception
  - deep learning
excerpt: "Our fMRI and behavioral study published in PNAS reveals that human visual cortex distinctly represents locomotive affordances in scenesâ€”beyond what current DNNs can model."
image:
  path: assets/images/michael-olsen-YWxfEBlvoiI-unsplash.jpg
  alt: "Thumbnail of fMRI study on action affordances"
header:
  overlay_image: assets/images/michael-olsen-YWxfEBlvoiI-unsplash.jpg
  caption: "Photo by Michael Olsen on Unsplash"
  overlay_filter: 0.5
  actions:
    - label: "Read Abstract"
      url: "https://doi.org/10.1073/pnas.2411683121"
    - label: "Download PDF"
      url: "/assets/papers/2024-bartnik-affordances-fmri.pdf"
---

## Overview  
Our **fMRI and behavioral study**, published in *PNAS*, shows that **scene-selective areas of the brain â€” PPA and OPA â€” distinctly represent locomotive action affordances**, such as walking, biking, or swimming. These representations go beyond recognizing objects or surfaces and reflect the possibilities for movement that a scene affords.  

By combining **behavioral ratings**, **multi-voxel fMRI data**, and **deep neural network (DNN) models**, we investigated how humans perceive these action opportunities â€” and compared them to what current AI models can capture.  

---
<p align="center">
  <img src="/assets/images/fmri_website_image.png" alt="fMRI overview of affordance representations" width="1000"><br>
  <em>fMRI reveals distinct affordance representations in PPA and OPA, beyond objects and surfaces.</em>
</p>

## Why this matters  
 **Key finding**: The human visual cortex encodes affordances in a **task-independent and distinct way**, not explained by objects or materials alone.  

 **AI models fall short**: State-of-the-art DNNs trained on object or scene recognition (e.g., CLIP, ViTs) could only partially explain human brain representations. Encouragingly, **multimodal models like GPT-4**, when prompted appropriately, show potential for better alignment.  

---

<p align="center">
  <img src="/assets/images/fmri_website_image.png" alt="fMRI overview of affordance representations" width="1000"><br>
  <em>fMRI reveals distinct affordance representations in PPA and OPA, beyond objects and surfaces.</em>
</p>

---

## Further reading  
This research was also featured in a **press release from the University of Amsterdam**:  
 [What the human brain can do that AI canâ€™t](https://www.uva.nl/en/content/news/press-releases/2025/06/what-the-human-brain-can-do-that-ai-cant.html)  

ðŸ”— [Read the abstract at PNAS](https://doi.org/10.1073/pnas.2411683121)  
ðŸ“„ [Download the PDF](/assets/papers/2024-bartnik-affordances-fmri.pdf)  

---

This work expands our understanding of **affordance perception** and lays the groundwork for **more human-aligned visual AI models**.  
