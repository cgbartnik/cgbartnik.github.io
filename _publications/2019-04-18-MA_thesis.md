---
title: "Facial expression recognition (Master’s thesis)"
date: 2025-04-27
layout: single
categories:
  - research
tags:
  - master-thesis
  - facial-expression-recognition
  - cnn
  - xai
  - eye-tracking
excerpt: "Head-to-head comparison of where a CNN and humans look when recognizing facial expressions — and why they disagree."
image:
  path: /assets/images/projects/facial-exp-thesis/teaser.jpg
  alt: "Facial expression recognition thesis teaser"
header:
  overlay_image: /assets/images/kiko-camaclang-9JQ9aLoVh1s-unsplash.jpg
  caption: "Photo by Kiko Camaclang on Unsplash"
  overlay_filter: 0.45
  teaser: /assets/images/MA_thesis_overview.png
actions:
  - label: "Download PDF"
    url: /assets/papers/bartnik_master_thesis_facial_expression_recognition.pdf
  - label: "View on Google Drive"
    url: "https://your-public-drive-link.example.com"
---

# A comparison of visual saliency in humans and CNNs for facial expressions

**Published:** May 27, 2020 — Otto-Friedrich-University Bamberg  
**Author:** Clemens Georg Bartnik — **Assessor:** Prof. Dr. Ute Schmid  
**Title:** *A Comparison between Visual Saliency Maps of Convolutional Neural Networks and those of Human Beings: A Study on Facial Expression Recognition*

---

## Introduction
This work asks a simple question with big implications: *Do convolutional neural networks (CNNs) look at the same facial regions as humans when classifying emotions?*  
I fine-tuned a VGG-Face–based CNN to recognize the six basic emotions (plus neutral) and compared its **Layer-wise Relevance Propagation (LRP)** saliency maps to **human gaze heatmaps** from an eye-tracking study with 28 participants viewing the same KDEF images.

---


<p align="center">
  <img src="/assets/images/MA_thesis_overview.png" alt="Overview of Thesis findings" width="1000"><br>
  <em>Comparison of human gaze patterns and CNN saliency maps for facial expression recognition. 
  Eye-tracking (top row) highlights diagnostic facial regions, while LRP (middle) reveals CNN focus. 
  Perturbation analysis (bottom right) shows that CNN and human cues diverge, despite comparable accuracy.</em>
</p>


## Why it matters
Trust in AI systems depends on *how* they reach decisions, not just *how often* they’re right. If machines and humans rely on different cues, models may generalize poorly, be brittle to spurious signals, or conflict with domain knowledge—especially in sensitive settings (e.g., HCI, healthcare, safety-critical UX). This thesis quantifies those differences directly rather than inferring them indirectly.

---

## Methods at a glance
- **Data:** KDEF faces (frontal views; 7 expressions).  
- **Model:** VGG-Face feature extractor, fine-tuned classifier head for emotion labels.  
- **Explanations:** **LRP** saliency maps for CNN relevance.  
- **Humans:** Eye-tracking at 1000 Hz; KDE-based heatmaps from correct trials.  
- **Comparison:** **Perturbation analysis**—progressively occlude tiles in order of (human vs. CNN) saliency; measure accuracy drop vs. a random baseline; compute AUC differences.

---

## Key findings (short version)
- **Different hotspots:** CNN attention often diverged from human fixations across expressions; the two systems did **not** consistently emphasize the same facial regions.  
- **Class-specific effects:** Some emotions (e.g., happy vs. afraid) contributed more to the divergence than others—mirroring known human confusions and model biases.  
- **Reasonable accuracy, different reasoning:** The fine-tuned model achieved good test accuracy overall, yet **how** it achieved it differed from human visual strategies.

---

## Takeaways
1. **Performance ≠ alignment:** Comparable accuracy can mask misaligned evidence use between humans and models.  
2. **Explanations enable targeted fixes:** LRP + perturbation pinpoint which regions drive decisions, suggesting data augmentation or loss shaping to re-align cues.  
3. **Design for agreement when needed:** In user-facing systems, encouraging overlap with human-diagnostic regions may improve robustness and trust.

---

## Download
- **PDF:** [Master’s thesis (PDF)](/assets/papers/bartnik_master_thesis_facial_expression_recognition.pdf) — 105 pages, figures, appendices. 
---
