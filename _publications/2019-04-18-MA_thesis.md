---
title: "Facial expression recognition (Master’s thesis)"
date: 2025-04-27
layout: single
categories:
  - research
tags:
  - master-thesis
  - facial-expression-recognition
  - cnn
  - xai
  - eye-tracking
excerpt: "Head-to-head comparison of where a CNN and humans look when recognizing facial expressions — and why they disagree."
image:
  path: /assets/images/projects/facial-exp-thesis/teaser.jpg
  alt: "Facial expression recognition thesis teaser"
header:
  overlay_image: /assets/images/kiko-camaclang-9JQ9aLoVh1s-unsplash.jpg
  caption: "Photo by Kiko Camaclang on Unsplash"
  overlay_filter: 0.45
  teaser: /assets/images/MA_thesis_overview.png
actions:
  - label: "View on Google Drive"
    url: "https://drive.google.com/file/d/1-6dVYBBafSuqi7w-o7QYpZ-ZYJAa6Ter/view?usp=sharing"
---

# A comparison of visual saliency in humans and CNNs for facial expressions

**Date:** May 27, 2020 — Otto-Friedrich-University Bamberg  
**Author:** Clemens Georg Bartnik — **Assessor:** Prof. Dr. Ute Schmid  
**Title:** *A Comparison between Visual Saliency Maps of Convolutional Neural Networks and those of Human Beings: A Study on Facial Expression Recognition*

---

## Introduction
This work asks a simple question with big implications: *Do convolutional neural networks (CNNs) look at the same facial features as humans when classifying emotions?*  
I fine-tuned a VGG-Face–based CNN to recognize the six basic emotions (plus neutral) and used **Layer-wise Relevance Propagation (LRP)**, an explainable AI method, to generate saliency maps, which I then compared to **human gaze heatmaps** from an eye-tracking study with 28 participants viewing the same KDEF images.

---


<p align="center">
  <img src="/assets/images/MA_thesis_overview.png" alt="Overview of Thesis findings" width="1000"><br>
  <em>Comparison of human gaze patterns and CNN saliency maps for facial expression recognition. 
  Eye-tracking (top row) highlights diagnostic facial regions, while LRP (middle) reveals CNN focus. 
  Perturbation analysis (bottom right) shows that CNN and human cues diverge, despite comparable accuracy.</em>
</p>


## Why it matters
Trust in AI systems depends on *how* they reach decisions, not just *how often* they’re right. If machines and humans rely on different features, models may generalize poorly, be prone to spurious signals, or conflict with domain knowledge—especially in sensitive settings (e.g., HCI, healthcare, safety-critical UX). This thesis tires to utilize explainable AI algorithms and different methods like perturbation analysis to quantify the differences.

---

## Methods at a glance
- **Data:** KDEF faces (frontal views; 7 expressions).  
- **Model:** VGG-Face feature extractor, fine-tuned classifier head for emotion labels.  
- **Explanations:** **LRP** saliency maps for CNN relevance.  
- **Humans:** Eye-tracking at 1000 Hz; KDE-based heatmaps from correct trials.  
- **Comparison:** **Perturbation analysis**—progressively occlude tiles in order of (human vs. CNN) saliency; measure accuracy drop vs. a random baseline; compute AUC differences.

---

## Key finding 
- **Different features are used:** CNNs appear to rely on different features than humans. A caveat, however, is that gaze-based heatmaps are highly similar across emotions and mainly reflect a consistent face-scanning strategy, making them an imperfect behavioral measure for direct comparison with CNNs. Still, our results indicate that perturbation analysis provides a suitable tool for comparing human and machine processing.  
---

## Download
- **PDF:** [Master’s thesis (PDF)](https://drive.google.com/file/d/1-6dVYBBafSuqi7w-o7QYpZ-ZYJAa6Ter/view?usp=sharing) — 105 pages, figures, appendices. 
---
